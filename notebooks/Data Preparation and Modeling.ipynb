{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33c6770f885ae2b",
   "metadata": {},
   "source": [
    "# 3 Data Preparation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddad1662101bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Find project root (folder that contains \"data\")\n",
    "def get_project_root():\n",
    "    p = Path.cwd()\n",
    "    while not (p / \"data\").exists() and p != p.parent:\n",
    "        p = p.parent\n",
    "    return p\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "filename=\"titanic1.csv\"\n",
    "input_path = RAW_DATA_DIR / filename\n",
    "print(\"Reading from:\", input_path)  # optional but useful\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af1f7f52773df6",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/olanrewajurasheed/titanic-dataset-with-gradient-boosting\n",
    "https://www.kaggle.com/code/dmilla/introduction-to-decision-trees-titanic-dataset\n",
    "https://www.kaggle.com/code/sandhyakrishnan02/logistic-regression-titanic-dataset\n",
    "https://gemini.google.com/app/70d90cac352bc787\n",
    "https://www.kaggle.com/code/atuljhasg/titanic-top-3-models\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab7fa67be7edfa",
   "metadata": {},
   "source": [
    "# 3.1  Select data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de1eec8bd169ff",
   "metadata": {},
   "source": [
    "For modeling, I keep passenger attributes that could influence survival which are Age, Fare, Sex, family relations, class, embarkation and surival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7be5f494b3334",
   "metadata": {},
   "source": [
    "In this phase, I am going to prepare the data for the modeling part. \n",
    "* Removing redundant data like zeros or passenger id\n",
    "* Adding values to fields with one \n",
    "* Fill empty fields with the most frequent atribute from the category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9cbb438c78d8f",
   "metadata": {},
   "source": [
    "# 3.2 Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9e771bb06b326",
   "metadata": {},
   "source": [
    "#### Drop zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de066925c3e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(19):\n",
    "    zero = 'zero' + '.' + str(x)\n",
    "    df = df.drop(columns=zero, errors=\"ignore\")\n",
    "    \n",
    "df = df.drop(columns='zero', errors=\"ignore\")\n",
    "col  = df.filter(regex=r'(?i)^zero(\\.|$)(?:[0-9]|1[0-8])?$').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b590bad027397",
   "metadata": {},
   "source": [
    "I removed the zero* columns because they contained no meaningful information. The columns are filled with constant zeros and provide no predictive information for survival; excluding them reduces noise and improves interpretability without affecting model validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf6603f6e1fb76",
   "metadata": {},
   "source": [
    "#### Drop passangerid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ffda755f65086",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns='Passengerid', errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d3a61a622a482",
   "metadata": {},
   "source": [
    "I excluded Passengerid because it is an identifier rather than a passenger attribute. It doesn't carry any  causal or behavioral information which might affect probability on survival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627cd84ac88ff62",
   "metadata": {},
   "source": [
    "#### Rename 2urvive to Survive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65990de56b3a6b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"2urvived\": \"Survived\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55c9877a68c692",
   "metadata": {},
   "source": [
    "I renamed 2urvived to Survived to have clear names for rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5127bb3556ca16",
   "metadata": {},
   "source": [
    "# 3.3 Construct Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51beda013bd54e",
   "metadata": {},
   "source": [
    "#### Fill empty fields with the most frequent atribute from the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0a3825bf73a79",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df[df[\"Embarked\"].isna()]   \n",
    "mode_embarked = df[\"Embarked\"].mode()[0]\n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna(mode_embarked)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b069e4b1656aba",
   "metadata": {},
   "source": [
    "I filled in the missing Embarked values with the most common Embarked category because it’s a category column and only 2 rows were missing, which should have no effect on the probability of survival. Additionally, this keeps all passengers in the dataset, without having to delete any. \n",
    "\n",
    "And check last time for empty fields;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139281400c5e8b1f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a52d947cd1d32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09cb359e1a9ac34",
   "metadata": {},
   "source": [
    "# for frature purpoese this is family size and is_alione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42183a9961f01e9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "#df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e402cdc3e162c",
   "metadata": {},
   "source": [
    "# 3.4 Format \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aaf59736e7ee84",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a43a706b8102",
   "metadata": {},
   "source": [
    "Right now my dataset uses ordinal encoding for categorical features. This means each category is  a number like Embarked is 0/1/2. The problem is that this can create a fake order between categories and the model may treat 2 as “more” than 1, even though ports have no ranking. This is especially an issue for Logistic Regression, because it treats numeric values as ordered quantities.\n",
    "\n",
    "I convert the nominal categorical features to **one-hot encoding**, where each category becomes its own 0/1 column (e.g., Embarked_1, Embarked_2, Embarked_0). This prevents the model from assuming an order and makes the input meaning correct and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1a124c03a9734",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode Embarked \n",
    "df = pd.get_dummies(df, columns=['Embarked'], prefix='Embarked')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e44b7bd790260",
   "metadata": {},
   "source": [
    "### Model to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c84834d55e5e32",
   "metadata": {},
   "source": [
    "\n",
    "Titanic is a supervised learning problem, specifically binary classification\n",
    "\n",
    "* Supervised because OF labels: survived = 0 or 1.\n",
    "* Binary classification because there are exactly two outcomes\n",
    "* It’s tabular data: rows = passengers, columns = features\n",
    "\n",
    "Based on this I can choose a supervised binary classification problem on tabular data.\n",
    "\n",
    "* **Logistic Regression** It is usually the first model people try for a yes or no prediction. It’s quick to train, gives consistent results, and it’s easy to understand. It tells you which passenger details push the chance of survival up or down. It works best when each detail affects survival in a fairly straightforward way, for example, being in a higher class helps, being older can hurt.\n",
    "\n",
    "* **Decision Tree** Predicts survival using a flowchart of simple if–then questions (e.g., Sex?, Pclass?, Age threshold?). It splits passengers into smaller groups so that each group contains people with similar outcomes, and the final prediction is based on the most common outcome in the last group. Trees are useful because they can capture clear rules and feature combinations, but a single tree can overfit by becoming too detailed, so its depth and minimum leaf size should be limited.\n",
    "\n",
    "* **Random Forest** The main point is that it builds many decision trees, each trained on a different random sample of the passengers and a random subset of features. The final prediction is made by combining the trees votes. This reduces the chance that the model memorizes noise from the training data, so it usually generalizes better and produces more reliable results than a single decision tree on tabular datasets.\n",
    "\n",
    "* **Gradient Boosting** It is strong model by adding many small decision trees one at a time. It starts with a simple first tree that gives predictions. Then it checks where the model is wrong, and trains the next tree to focus mainly on fixing those mistakes. Each new tree is added to the previous ones, so the final prediction is the sum of all trees contributions. Because it improves step by step, Gradient Boosting often achieves very high accuracy on tabular data, but it is harder to follow, how the outcome has been computed. \n",
    "\n",
    "All models require numeric input features, so the dataset must not contain raw text categories and must not contain missing values at training time. There is no need to use balanced versions for categories in model. As in titanic different categories, etc. have higher or lower values.\n",
    "\n",
    "# make it better here\n",
    "1) K-Nearest Neighbors (KNN)\n",
    "\n",
    "Why it could work: simple non-parametric classifier; can capture non-linear boundaries.\n",
    "Why we don’t prioritize it: sensitive to feature scaling and irrelevant features; performance can be unstable on mixed-type tabular data; less interpretable for stakeholders than tree rules or LR coefficients.\n",
    "\n",
    "2) Support Vector Machine (SVM)\n",
    "\n",
    "Why it could work: strong classifier, especially with non-linear kernels (RBF).\n",
    "Why we don’t prioritize it: needs careful scaling and tuning (C, gamma), can be slower, and is harder to explain than LR/trees; probability outputs require extra steps.\n",
    "\n",
    "3) Naive Bayes\n",
    "\n",
    "Why it could work: fast, good baseline in some domains.\n",
    "Why we don’t prioritize it: relies on strong independence assumptions between features (unlikely for Titanic: Pclass, Fare, Embarked are correlated), so it often underperforms.\n",
    "\n",
    "4) Neural Networks (MLP)\n",
    "\n",
    "Why it could work: flexible; can model complex interactions.\n",
    "Why we don’t prioritize it: overkill for a small tabular dataset; needs more tuning and regularization; less interpretable and less stable for this assignment scope.\n",
    "\n",
    "5) Extremely Randomized Trees (ExtraTrees)\n",
    "\n",
    "Why it could work: often comparable to Random Forest, sometimes faster/stronger.\n",
    "Why we didn’t choose it: very similar story to Random Forest, so it adds less educational value than including Gradient Boosting as a different ensemble strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae6fe7cc3672ea",
   "metadata": {},
   "source": [
    "### Target and Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7ea2cd534502d",
   "metadata": {},
   "source": [
    "After cleaning the dataset and handling missing values, I prepared it for modeling by separating the target from the input features. Survived is the target variable \n",
    "y, and the remaining passenger attributes form the feature matrix  X. I then split the data into training and test sets. The training set is used to fit the models, while the test set is kept for evaluating performance on unseen passengers. I used a stratified split to keep the proportion of survivors and non-survivors similar in both sets, and a fixed random state to ensure the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc53767ca3d4dd1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"Survived\"].astype(int)\n",
    "X = df.drop(columns=[\"Survived\"], errors=\"ignore\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaf776b51f28e9",
   "metadata": {},
   "source": [
    "# 4 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd278a1e1efee9d",
   "metadata": {},
   "source": [
    "  The Titanic dataset is tabular with a mix of numeric and categorical variables, so categorical features must be encoded appropriately. The goal is to predict survival (0/1), which is a supervised binary classification task. Therefore, I selected classification models suitable for structured data and compared an interpretable baseline (Logistic Regression), an interpretable non-linear model (Decision Tree), and two ensemble tree methods (Random Forest and XGBoost) to balance explainability and predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e99e6cffb2bf4",
   "metadata": {},
   "source": [
    "### Measuring the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754462dc368d2df9",
   "metadata": {},
   "source": [
    "The outcome will be mearured using these: \n",
    "ROC-AUC (overall ability to separate classes)\n",
    "F1-score (balance between precision and recall)\n",
    "Confusion matrix (what errors you make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dde2e11e49c07",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def evaluate(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)[:, 1]  # works for all 4 here\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_test, pred), 4))\n",
    "    print(\"ROC-AUC :\", round(roc_auc_score(y_test, proba), 4))\n",
    "    print(\"F1      :\", round(f1_score(y_test, pred), 4))\n",
    "\n",
    "# 2) models\n",
    "evaluate(LogisticRegression(max_iter=4000), \"Logistic Regression\")\n",
    "\n",
    "evaluate(DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_leaf=5), \"Decision Tree\")\n",
    "\n",
    "evaluate(LogisticRegression(max_iter=4000,  class_weight=\"balanced\"),\"LogisticRegression but balanced\")\n",
    "\n",
    "evaluate(RandomForestClassifier(random_state=42, n_estimators=300, max_depth=None), \"Random Forest\")\n",
    "\n",
    "evaluate(XGBClassifier(random_state=42,n_estimators=300,learning_rate=0.05,max_depth=4,subsample=0.8,colsample_bytree=0.8,eval_metric=\"logloss\"),\n",
    "         \"XGBoost (Gradient Boosting)\")\n",
    "\n",
    "evaluate(GradientBoostingClassifier(n_estimators=100,learning_rate=0.05),\"GradientBoostingClassifier\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1117ca18fd8ce9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb6de5fa32eb75c9",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc367906dba44c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c063e4aa780b4f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
